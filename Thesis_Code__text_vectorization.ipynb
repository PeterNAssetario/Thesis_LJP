{"cells":[{"cell_type":"markdown","metadata":{"id":"a4b5da75-708d-442f-b2e5-a35db4dcd770"},"source":["# Packages:"],"id":"a4b5da75-708d-442f-b2e5-a35db4dcd770"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFNsRyLQkgzL"},"outputs":[],"source":["try:\n","  from google.colab import drive\n","  !nvidia-smi\n","  drive.mount('/content/drive')\n","  path = 'drive/MyDrive/Thesis/'\n","except:\n","  path = './'"],"id":"aFNsRyLQkgzL"},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"id":"RaS62pNGDAtP"},"id":"RaS62pNGDAtP","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3d0caca-2f9e-4e45-9f29-de0ddcb91f0b"},"outputs":[],"source":["# Packages for loading data:\n","from os import walk\n","import os\n","import pprint\n","import itertools\n","import json\n","import re\n","import pickle\n","import sys\n","import warnings\n","\n","# Packages for effective data storage / math utils:\n","import pandas as pd\n","import numpy as np\n","\n","# Packages for plotting:\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Packages for text representation:\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n","from gensim.models import Word2Vec, Doc2Vec\n","\n","# Misc.:\n","import time\n","import multiprocessing\n","\n","seed = 101\n","cores = multiprocessing.cpu_count()"],"id":"c3d0caca-2f9e-4e45-9f29-de0ddcb91f0b"},{"cell_type":"markdown","metadata":{"id":"17e93e78-98d6-428d-93e7-e725236b7ae7"},"source":["# Text Representation (For classical ML)"],"id":"17e93e78-98d6-428d-93e7-e725236b7ae7"},{"cell_type":"markdown","metadata":{"id":"29798263-2f2c-4714-8685-0c4d0597fb76"},"source":["* Bag-of-ngarms:\n","  * ngram (1, 1) (- PCA) (+ Truncated SVD) (+ LDA)\n","  * ngram (1, 2) (- PCA) (+ Truncated SVD) (+ LDA)\n","* TF-IDF:\n","  * ngram (1, 1) (- PCA) (+ Truncated SVD) (+ LDA)\n","  * ngram (1, 2) (- PCA) (+ Truncated SVD) (+ LDA)"],"id":"29798263-2f2c-4714-8685-0c4d0597fb76"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687766315315,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"n66kpfg3plHx","outputId":"2c3f1a4b-0cb0-4642-e15c-c4770705a054"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'drive/MyDrive/Thesis/'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["path"],"id":"n66kpfg3plHx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"511c243f-8e54-4b1a-b4ac-4ba6d09226c5"},"outputs":[],"source":["df_train_x = pd.read_pickle(path + \"ECHR_Dataset_clean/df_train_x.pkl\")\n","df_train_y = pd.read_pickle(path + \"ECHR_Dataset_clean/df_train_y.pkl\")\n","df_test_x  = pd.read_pickle(path + \"ECHR_Dataset_clean/df_test_x.pkl\")\n","df_test_y  = pd.read_pickle(path + \"ECHR_Dataset_clean/df_test_y.pkl\")"],"id":"511c243f-8e54-4b1a-b4ac-4ba6d09226c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"09d1a121-0d69-4c9f-9bef-614fe08b0d93"},"outputs":[],"source":["def text_rep_pipe(\n","    corpus_train,\n","    corpus_test,\n","    ngram_size,\n","    vectorizer,\n","    dim_red_method = None,\n","):\n","    if vectorizer == 'BoW':\n","        vec = CountVectorizer(\n","            ngram_range=(1, ngram_size),\n","            min_df = 3,\n","            max_df = 0.95,\n","            dtype =  np.int16,\n","        )\n","    elif vectorizer == 'TFIDF':\n","        vec = TfidfVectorizer(\n","            ngram_range=(1, ngram_size),\n","            min_df = 3,\n","            max_df = 0.95,\n","            dtype = np.float32\n","        )\n","    else:\n","        return(\"Wrong vectorizer input\")\n","    bow_matrix_train = abs(vec.fit_transform(corpus_train))\n","    bow_array_train = bow_matrix_train.toarray()\n","\n","    bow_matrix_test = vec.transform(corpus_test)\n","    bow_array_test = bow_matrix_test.toarray()\n","\n","    print(\"Vec Done\")\n","\n","    #if dim_red_method == 'PCA':\n","    #    pca_algo = PCA(n_components = 0.95, svd_solver = 'full')\n","    #    pca_train = pca_algo.fit_transform(bow_array_train)\n","    #    bow_df_train = pd.DataFrame(data=pca_train)\n","    #\n","    #    pca_test = pca_algo.transform(bow_array_test)\n","    #    bow_df_test = pd.DataFrame(data=pca_test)\n","\n","    if dim_red_method == 'tSVD':\n","        tsvd_algo = TruncatedSVD(algorithm = 'randomized', n_components = 3000)\n","        tsvd_train = tsvd_algo.fit_transform(bow_matrix_train.asfptype())\n","        bow_df_train = pd.DataFrame(data=tsvd_train)\n","\n","        tsvd_test = tsvd_algo.transform(bow_array_test)\n","        bow_df_test = pd.DataFrame(data=tsvd_test)\n","\n","    elif dim_red_method == 'LDA':\n","        # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4597325/\n","        if vectorizer == 'BoW': n_topics = 35\n","        elif vectorizer == 'TFIDF': n_topics = 5\n","        lda_algo = LatentDirichletAllocation(learning_method = 'online', n_components = n_topics)\n","        lda_train = lda_algo.fit_transform(bow_array_train)\n","        bow_df_train = pd.DataFrame(data=lda_train)\n","\n","        lda_test = lda_algo.transform(bow_array_test)\n","        bow_df_test = pd.DataFrame(data=lda_test)\n","\n","    else:\n","        bow_df_train = pd.DataFrame(data=bow_array_train, columns = vec.get_feature_names_out())\n","        bow_df_test = pd.DataFrame(data=bow_array_test, columns = vec.get_feature_names_out())\n","\n","    print(\"Dim Red Done\")\n","\n","    return(bow_df_train, bow_df_test)"],"id":"09d1a121-0d69-4c9f-9bef-614fe08b0d93"},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0Z6Wt3q4aFr"},"outputs":[],"source":["# Getting n_components for tSVD\n","vec = TfidfVectorizer(ngram_range=(1, 2), min_df = 6, max_df = 0.9, dtype = np.float32)\n","bow_matrix_train = vec.fit_transform(df_train_x)\n","bow_array_train = bow_matrix_train.toarray()\n","bow_df_train = pd.DataFrame(data=bow_array_train, columns = vec.get_feature_names_out())\n","\n","tsvd_algo = TruncatedSVD(algorithm = 'randomized', n_components = 2500)\n","tsvd_train = tsvd_algo.fit_transform(bow_matrix_train.asfptype())\n","\n","cumsum_exp_var = np.cumsum(tsvd_algo.explained_variance_ratio_)\n","cumsum_exp_var\n","\n","n_th_comp = next(x[0] for x in enumerate(cumsum_exp_var) if x[1] > 0.8) + 1\n","n_th_comp # 80%"],"id":"x0Z6Wt3q4aFr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-h8YTPlwJdM"},"outputs":[],"source":["vec = TfidfVectorizer(ngram_range=(1, 1), min_df = 3, max_df = 0.95, dtype = np.float32)\n","bow_matrix_train = vec.fit_transform(df_train_x)\n","bow_array_train = bow_matrix_train.toarray()\n","bow_df_train = pd.DataFrame(data=bow_array_train, columns = vec.get_feature_names_out())\n","\n","x = [5, 10, 15, 20, 25]\n","log_lik = []\n","for n_topics in x:\n","    lda_algo = LatentDirichletAllocation(learning_method = 'online', n_components = n_topics)\n","    lda_train = lda_algo.fit_transform(bow_array_train)\n","    log_lik.append(lda_algo.score(bow_array_train))\n","\n","print(log_lik, np.argmax(log_lik))"],"id":"z-h8YTPlwJdM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54867,"status":"ok","timestamp":1687766542110,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"c1285bb8-de29-4b0e-bb16-7e2ae60cb4bd","outputId":"fec6dabc-ba05-4b3c-9784-157dab6cea5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vec Done\n","Dim Red Done\n"]}],"source":["#bow_uni_train_x,      bow_uni_test_x          = text_rep_pipe(df_train_x, df_test_x, 1, 'BoW', None)\n","#bow_uni_train_x_tsvd, bow_uni_test_x_tsvd     = text_rep_pipe(df_train_x, df_test_x, 1, 'BoW', 'tSVD')\n","#bow_uni_train_x_lda,  bow_uni_test_x_lda      = text_rep_pipe(df_train_x, df_test_x, 1, 'BoW', 'LDA')\n","\n","#bow_bi_train_x,      bow_bi_test_x            = text_rep_pipe(df_train_x, df_test_x, 2, 'BoW', None)\n","#bow_bi_train_x_tsvd, bow_bi_test_x_tsvd       = text_rep_pipe(df_train_x, df_test_x, 2, 'BoW', 'tSVD')\n","#bow_bi_train_x_lda,  bow_bi_test_x_lda        = text_rep_pipe(df_train_x, df_test_x, 2, 'BoW', 'LDA')\n","\n","\n","#tfidf_uni_train_x,      tfidf_uni_test_x      = text_rep_pipe(df_train_x, df_test_x, 1, 'TFIDF', None)\n","#tfidf_uni_train_x_tsvd, tfidf_uni_test_x_tsvd = text_rep_pipe(df_train_x, df_test_x, 1, 'TFIDF', 'tSVD')\n","#tfidf_uni_train_x_lda,  tfidf_uni_test_x_lda  = text_rep_pipe(df_train_x, df_test_x, 1, 'TFIDF', 'LDA')\n","\n","tfidf_bi_train_x,      tfidf_bi_test_x        = text_rep_pipe(df_train_x, df_test_x, 2, 'TFIDF', None)\n","#tfidf_bi_train_x_tsvd, tfidf_bi_test_x_tsvd   = text_rep_pipe(df_train_x, df_test_x, 2, 'TFIDF', 'tSVD')\n","#tfidf_bi_train_x_lda,  tfidf_bi_test_x_lda    = text_rep_pipe(df_train_x, df_test_x, 2, 'TFIDF', 'LDA')"],"id":"c1285bb8-de29-4b0e-bb16-7e2ae60cb4bd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdc7b461-e139-44e4-802b-3811318628fc"},"outputs":[],"source":["tfidf_bi_train_x_tsvd.columns = tfidf_bi_train_x_tsvd.columns.map(str)\n","tfidf_bi_test_x_tsvd.columns  = tfidf_bi_test_x_tsvd.columns.map(str)"],"id":"bdc7b461-e139-44e4-802b-3811318628fc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ae5fe3e-8ef1-4318-94f4-281521917ffd"},"outputs":[],"source":["#bow_uni_train_x.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_train_x.parquet.gzip\", compression='gzip', index = False)\n","#bow_uni_test_x.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_test_x.parquet.gzip\", compression='gzip', index = False)\n","#bow_uni_train_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_train_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#bow_uni_test_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_test_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#bow_uni_train_x_lda.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_train_x_lda.parquet.gzip\", compression='gzip', index = False)\n","#bow_uni_test_x_lda.to_parquet(path + \"ECHR_Dataset_vec/bow_uni_test_x_lda.parquet.gzip\", compression='gzip', index = False)\n","\n","#bow_bi_train_x.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_train_x.parquet.gzip\", compression='gzip', index = False)\n","#bow_bi_test_x.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_test_x.parquet.gzip\", compression='gzip', index = False)\n","#bow_bi_train_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_train_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#bow_bi_test_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_test_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#bow_bi_train_x_lda.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_train_x_lda.parquet.gzip\", compression='gzip', index = False)\n","#bow_bi_test_x_lda.to_parquet(path + \"ECHR_Dataset_vec/bow_bi_test_x_lda.parquet.gzip\", compression='gzip', index = False)\n","\n","\n","#tfidf_uni_train_x.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_train_x.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_uni_test_x.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_test_x.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_uni_train_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_train_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_uni_test_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_test_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_uni_train_x_lda.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_train_x_lda.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_uni_test_x_lda.to_parquet(path + \"ECHR_Dataset_vec/tfidf_uni_test_x_lda.parquet.gzip\", compression='gzip', index = False)\n","\n","#tfidf_bi_train_x.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_train_x.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_bi_test_x.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_test_x.parquet.gzip\", compression='gzip', index = False)\n","tfidf_bi_train_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_train_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","tfidf_bi_test_x_tsvd.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_test_x_tsvd.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_bi_train_x_lda.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_train_x_lda.parquet.gzip\", compression='gzip', index = False)\n","#tfidf_bi_test_x_lda.to_parquet(path + \"ECHR_Dataset_vec/tfidf_bi_test_x_lda.parquet.gzip\", compression='gzip', index = False)"],"id":"8ae5fe3e-8ef1-4318-94f4-281521917ffd"},{"cell_type":"markdown","metadata":{"id":"ce3f82f3-0784-4845-849f-3d141728a11d"},"source":["* ...2Vec algos\n","  * Word2Vec\n","  * Doc2Vec"],"id":"ce3f82f3-0784-4845-849f-3d141728a11d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270805,"status":"ok","timestamp":1686227993490,"user":{"displayName":"Ester Perdochova","userId":"01707098838272220978"},"user_tz":-120},"id":"bba85086-80ee-4555-b65d-5f47a0cc6d9d","outputId":"4328fd25-1bfc-4284-c101-c21b0a6b29a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-11-7b82b164fcba>:14: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n","  w2v_model.init_sims(\n","WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"]}],"source":["w2v_model = Word2Vec(\n","    min_count=3,\n","    workers=cores - 1\n",")\n","w2v_model.build_vocab(\n","    df_train_x.apply(lambda x: x.split(\" \"))\n",")\n","w2v_model.train(\n","    df_train_x.apply(lambda x: x.split(\" \")),\n","    total_examples = w2v_model.corpus_count,\n","    epochs = 30,\n","    report_delay=1\n",")\n","w2v_model.init_sims(\n","    replace=True\n",")"],"id":"bba85086-80ee-4555-b65d-5f47a0cc6d9d"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322309,"status":"ok","timestamp":1686228758269,"user":{"displayName":"Ester Perdochova","userId":"01707098838272220978"},"user_tz":-120},"id":"64c6facc-bf84-4227-a367-8a6cb23669c2","outputId":"a76db677-48df-40a8-c6e0-fcdbca8c9fe6"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-24-38ccaa53ceec>:16: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n","  d2v_model.init_sims(\n","WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"]}],"source":["corpus_iterable = [doc2vec.TaggedDocument(doc, [i]) for i, doc in enumerate(df_train_x.apply(lambda x: x.split(\" \")))]\n","\n","d2v_model = Doc2Vec(\n","    min_count=3,\n","    workers=cores - 1\n",")\n","d2v_model.build_vocab(\n","    corpus_iterable = corpus_iterable,\n",")\n","d2v_model.train(\n","    corpus_iterable = corpus_iterable,\n","    total_examples = w2v_model.corpus_count,\n","    epochs = 30,\n","    report_delay=1\n",")\n","d2v_model.init_sims(\n","    replace=True\n",")"],"id":"64c6facc-bf84-4227-a367-8a6cb23669c2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6616c5f4-b4b2-449a-aaf4-d5e10a5f9ba1"},"outputs":[],"source":["def w2v_vectorize(sentence):\n","    words = sentence.split(\" \")\n","    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n","    if len(words_vecs) == 0:\n","        return np.zeros(100)\n","    words_vecs = np.array(words_vecs)\n","    return words_vecs.mean(axis=0)\n","\n","def d2v_vectorize(document):\n","    words_vecs = d2v_model.infer_vector(document.split())\n","    words_vecs = np.array(words_vecs)\n","    return words_vecs"],"id":"6616c5f4-b4b2-449a-aaf4-d5e10a5f9ba1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4eb3b9cf-4719-4fd8-ab5a-e13484e7fdba"},"outputs":[],"source":["w2v_train_x = pd.DataFrame(np.array([w2v_vectorize(sentence) for sentence in df_train_x]))\n","w2v_test_x = pd.DataFrame(np.array([w2v_vectorize(sentence) for sentence in df_test_x]))\n","d2v_train_x = pd.DataFrame(np.array([d2v_vectorize(sentence) for sentence in df_train_x]))\n","d2v_test_x = pd.DataFrame(np.array([d2v_vectorize(sentence) for sentence in df_test_x]))"],"id":"4eb3b9cf-4719-4fd8-ab5a-e13484e7fdba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN6BXbtfBoSX"},"outputs":[],"source":["w2v_train_x.columns = w2v_train_x.columns.map(str)\n","w2v_test_x.columns  = d2v_tew2v_test_xst_x.columns.map(str)\n","d2v_train_x.columns = d2v_train_x.columns.map(str)\n","d2v_test_x.columns  = d2v_test_x.columns.map(str)"],"id":"KN6BXbtfBoSX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0fsHSRT_QLn"},"outputs":[],"source":["w2v_train_x.to_parquet(path + \"ECHR_Dataset_vec/w2v_train_x.parquet.gzip\", compression='gzip', index = False)\n","w2v_test_x.to_parquet(path + \"ECHR_Dataset_vec/w2v_test_x.parquet.gzip\", compression='gzip', index = False)\n","d2v_train_x.to_parquet(path + \"ECHR_Dataset_vec/d2v_train_x.parquet.gzip\", compression='gzip', index = False)\n","d2v_test_x.to_parquet(path + \"ECHR_Dataset_vec/d2v_test_x.parquet.gzip\", compression='gzip', index = False)"],"id":"x0fsHSRT_QLn"},{"cell_type":"markdown","metadata":{"id":"4a6fbf7e-be20-4a05-936c-22a083221b88"},"source":["* GloVe\n","  * Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n","  * Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download)"],"id":"4a6fbf7e-be20-4a05-936c-22a083221b88"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c70b0e2-5b6b-4a5a-b1e2-9d238212149a"},"outputs":[],"source":["def load_glove_vectors(glove_file):\n","    embeddings = {}\n","    with open(glove_file, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.strip().split()\n","            word = values[0]\n","            try:\n","                vector = np.asarray(values[1:], dtype='float32')\n","                embeddings[word] = vector\n","            except:\n","                _ = 1\n","    return embeddings\n","\n","def get_average_word_vectors(words_list, embeddings, vector_size=300):\n","    avg_word_vec = []\n","    for word in words_list:\n","        if word in embeddings:\n","            avg_word_vec.append(embeddings[word])\n","    if len(avg_word_vec) > 0:\n","        avg_word_vec = np.mean(avg_word_vec, axis=0)\n","    else:\n","        avg_word_vec = np.zeros(vector_size)\n","    return avg_word_vec\n","\n","# Load GloVe vectors:\n","glove_file = './glove.840B.300d.txt'\n","glove_embeddings = load_glove_vectors(glove_file)\n","\n","# Get average word vectors for training and test sets:\n","train_avg_word_vec = np.array([get_average_word_vectors(doc, glove_embeddings) for doc in [s.split() for s in df_train_x]])\n","test_avg_word_vec = np.array([get_average_word_vectors(doc, glove_embeddings) for doc in [s.split() for s in df_test_x]])"],"id":"6c70b0e2-5b6b-4a5a-b1e2-9d238212149a"},{"cell_type":"code","source":["glove_train_x = pd.DataFrame(train_avg_word_vec)\n","glove_test_x = pd.DataFrame(test_avg_word_vec)\n","glove_train_x.columns = glove_train_x.columns.map(str)\n","glove_test_x.columns  = glove_test_x.columns.map(str)"],"metadata":{"id":"9fxtN3niosVD"},"id":"9fxtN3niosVD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove_train_x.to_parquet(path + \"ECHR_Dataset_vec/glove_train_x.parquet.gzip\", compression='gzip', index = False)\n","glove_test_x.to_parquet(path + \"ECHR_Dataset_vec/glove_test_x.parquet.gzip\", compression='gzip', index = False)"],"metadata":{"id":"T7T1wsyDosSK"},"id":"T7T1wsyDosSK","execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}