{"cells":[{"cell_type":"markdown","metadata":{"id":"Y_JhK8UUQA--"},"source":["# Packages:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2068,"status":"ok","timestamp":1688310425194,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"9WmqjqM4PIEE","outputId":"66306ac7-d17e-454b-ca5b-079cec9cb95f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: nvidia-smi: command not found\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["try:\n","  from google.colab import drive\n","  !nvidia-smi\n","  drive.mount('/content/drive')\n","  path = 'drive/MyDrive/Thesis/'\n","except:\n","  path = './'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1467,"status":"ok","timestamp":1688310426659,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"rqtvO5bgPxyD"},"outputs":[],"source":["# Packages for loading data:\n","from os import walk\n","import itertools\n","import json\n","import re\n","import pickle\n","\n","# Packages for effective data storage / math utils:\n","import pandas as pd\n","import numpy as np\n","\n","# Packages for plotting:\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Packages for modeling:\n","from sklearn.linear_model import LogisticRegression, Perceptron\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neural_network import MLPClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","\n","# Packages for performance:\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","\n","# Misc.:\n","import time\n","import multiprocessing\n","\n","seed = 101\n","cores = multiprocessing.cpu_count()"]},{"cell_type":"markdown","metadata":{"id":"ABmVdjDYQDZ5"},"source":["## Model Evaluation:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1688310426660,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"Dr2RI3raQo0-"},"outputs":[],"source":["def vec_path_getter(\n","    vecpath : str,\n","    contains : str,\n","):\n","    #########\n","    # Input:\n","    # Output: list of all paths to jasons to be used later\n","    #########\n","\n","    filenames = next(walk(vecpath), (None, None, []))[2]\n","    filenames = [str(vecpath + \"/\" + file) for file in filenames\n","                 if contains in file]\n","\n","    return(filenames)\n","\n","# Load model, with correct test dataset -> predict on test -> return confusion matrix\n","vec_x_paths = vec_path_getter(\n","    path + \"ECHR_Dataset_vec\",\n","    \"x\"\n","    )\n","pattern = r'(?<=vec/)(.*?)(?=_test_x.parquet.gzip|_train_x.parquet.gzip)'\n","unique_datasets = list(set([re.search(pattern, string).group(0) for string in vec_x_paths]))\n","vec_x_paths = [[x for x in vec_x_paths if str(i + \"_train\") in x or str(i + \"_test\") in x] for i in unique_datasets]\n","vec_x_paths = [\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/w2v_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/w2v_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/d2v_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/d2v_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/glove_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/glove_test_x.parquet.gzip'],\n","    ]\n","\n","y_paths = vec_path_getter(\n","    path + \"ECHR_Dataset_clean\",\n","    \"y\"\n","    )\n","y_paths.sort()\n","\n","classical_models = [\n","    LogisticRegression,\n","    GaussianNB,\n","    SVC,\n","    DecisionTreeClassifier,\n","    RandomForestClassifier,\n","    LGBMClassifier,\n","    KNeighborsClassifier,\n","    Perceptron,\n","    MLPClassifier,\n","    ]"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZLS2DZ26RiwD","executionInfo":{"status":"ok","timestamp":1688310427462,"user_tz":-120,"elapsed":3,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}}},"outputs":[],"source":["def run_eval_on_models(models, datasets_paths):\n","    y_test =  pd.read_pickle([i for i in y_paths if 'test'  in i][0])\n","    pattern = r'(?<=vec/)(.*?)(?=_test_x.parquet.gzip|_train_x.parquet.gzip)'\n","\n","    # Create list of DataFrames to store the results:\n","    accuracy_dfs = []\n","    f1_dfs = []\n","    params_dfs = []\n","\n","    for i, model in enumerate(models):\n","        # Create temporary dictionaries to store the results for each dataset:\n","        accuracy_dict = {}\n","        f1_dict = {}\n","        params_dict = {}\n","        for j, datasets in enumerate(datasets_paths):\n","            # Set up:\n","            datasets.sort()\n","            temp_model_name = str(model).split('.')[-1].split(\"'\")[0]\n","            temp_data_name = re.search(pattern, datasets[0]).group(0)\n","            print(f\"Assesing model {temp_model_name} on dataset {temp_data_name}\")\n","\n","            # Read correct data:\n","            #x_train = pd.read_parquet(datasets[1])\n","            x_test = pd.read_parquet(datasets[0])\n","\n","            # Load best model - if it exists:\n","            try:\n","                filename = f\"{path}ECHR_model/model_{temp_model_name}__dataset_{temp_data_name}.pkl\"\n","                loaded_model = pickle.load(open(filename, 'rb'))\n","                y_pred = loaded_model.predict(x_test)\n","                params = loaded_model.get_params()\n","                accuracy = accuracy_score(y_test, y_pred)\n","                f1 = f1_score(y_test, y_pred, average='macro')\n","            except:\n","                params = np.nan\n","                accuracy = np.nan\n","                f1 = np.nan\n","\n","            # Store the results in the dictionaries:\n","            accuracy_dict[temp_data_name] = accuracy\n","            f1_dict[temp_data_name] = f1\n","            params_dict[temp_data_name] = params\n","\n","        # Add the results for this model to the DataFrames:\n","        accuracy_dfs.append(pd.Series(accuracy_dict, name=temp_model_name))\n","        f1_dfs.append(pd.Series(f1_dict, name=temp_model_name))\n","        params_dfs.append(pd.Series(params_dict, name=temp_model_name))\n","\n","\n","    # Concatenate all the DataFrames in the list:\n","    accuracy_df = pd.concat(accuracy_dfs, axis=1)\n","    f1_df = pd.concat(f1_dfs, axis=1)\n","    params_df = pd.concat(params_dfs, axis=1)\n","\n","    # Return the resulting DataFrames:\n","    return accuracy_df, f1_df, params_df"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zXTgZG66Tzug","executionInfo":{"status":"ok","timestamp":1688311177594,"user_tz":-120,"elapsed":744890,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}},"outputId":"58a5688a-132f-48a1-c7f0-a03e1498dffb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Assesing model LogisticRegression on dataset bow_uni_lda\n","Assesing model LogisticRegression on dataset bow_bi_lda\n","Assesing model LogisticRegression on dataset bow_uni_tsvd\n","Assesing model LogisticRegression on dataset bow_bi_tsvd\n","Assesing model LogisticRegression on dataset tfidf_uni_lda\n","Assesing model LogisticRegression on dataset tfidf_bi_lda\n","Assesing model LogisticRegression on dataset tfidf_uni_tsvd\n","Assesing model LogisticRegression on dataset tfidf_bi_tsvd\n","Assesing model LogisticRegression on dataset w2v\n","Assesing model LogisticRegression on dataset d2v\n","Assesing model LogisticRegression on dataset glove\n","Assesing model GaussianNB on dataset bow_uni_lda\n","Assesing model GaussianNB on dataset bow_bi_lda\n","Assesing model GaussianNB on dataset bow_uni_tsvd\n","Assesing model GaussianNB on dataset bow_bi_tsvd\n","Assesing model GaussianNB on dataset tfidf_uni_lda\n","Assesing model GaussianNB on dataset tfidf_bi_lda\n","Assesing model GaussianNB on dataset tfidf_uni_tsvd\n","Assesing model GaussianNB on dataset tfidf_bi_tsvd\n","Assesing model GaussianNB on dataset w2v\n","Assesing model GaussianNB on dataset d2v\n","Assesing model GaussianNB on dataset glove\n","Assesing model SVC on dataset bow_uni_lda\n","Assesing model SVC on dataset bow_bi_lda\n","Assesing model SVC on dataset bow_uni_tsvd\n","Assesing model SVC on dataset bow_bi_tsvd\n","Assesing model SVC on dataset tfidf_uni_lda\n","Assesing model SVC on dataset tfidf_bi_lda\n","Assesing model SVC on dataset tfidf_uni_tsvd\n","Assesing model SVC on dataset tfidf_bi_tsvd\n","Assesing model SVC on dataset w2v\n","Assesing model SVC on dataset d2v\n","Assesing model SVC on dataset glove\n","Assesing model DecisionTreeClassifier on dataset bow_uni_lda\n","Assesing model DecisionTreeClassifier on dataset bow_bi_lda\n","Assesing model DecisionTreeClassifier on dataset bow_uni_tsvd\n","Assesing model DecisionTreeClassifier on dataset bow_bi_tsvd\n","Assesing model DecisionTreeClassifier on dataset tfidf_uni_lda\n","Assesing model DecisionTreeClassifier on dataset tfidf_bi_lda\n","Assesing model DecisionTreeClassifier on dataset tfidf_uni_tsvd\n","Assesing model DecisionTreeClassifier on dataset tfidf_bi_tsvd\n","Assesing model DecisionTreeClassifier on dataset w2v\n","Assesing model DecisionTreeClassifier on dataset d2v\n","Assesing model DecisionTreeClassifier on dataset glove\n","Assesing model RandomForestClassifier on dataset bow_uni_lda\n","Assesing model RandomForestClassifier on dataset bow_bi_lda\n","Assesing model RandomForestClassifier on dataset bow_uni_tsvd\n","Assesing model RandomForestClassifier on dataset bow_bi_tsvd\n","Assesing model RandomForestClassifier on dataset tfidf_uni_lda\n","Assesing model RandomForestClassifier on dataset tfidf_bi_lda\n","Assesing model RandomForestClassifier on dataset tfidf_uni_tsvd\n","Assesing model RandomForestClassifier on dataset tfidf_bi_tsvd\n","Assesing model RandomForestClassifier on dataset w2v\n","Assesing model RandomForestClassifier on dataset d2v\n","Assesing model RandomForestClassifier on dataset glove\n","Assesing model LGBMClassifier on dataset bow_uni_lda\n","Assesing model LGBMClassifier on dataset bow_bi_lda\n","Assesing model LGBMClassifier on dataset bow_uni_tsvd\n","Assesing model LGBMClassifier on dataset bow_bi_tsvd\n","Assesing model LGBMClassifier on dataset tfidf_uni_lda\n","Assesing model LGBMClassifier on dataset tfidf_bi_lda\n","Assesing model LGBMClassifier on dataset tfidf_uni_tsvd\n","Assesing model LGBMClassifier on dataset tfidf_bi_tsvd\n","Assesing model LGBMClassifier on dataset w2v\n","Assesing model LGBMClassifier on dataset d2v\n","Assesing model LGBMClassifier on dataset glove\n","Assesing model KNeighborsClassifier on dataset bow_uni_lda\n","Assesing model KNeighborsClassifier on dataset bow_bi_lda\n","Assesing model KNeighborsClassifier on dataset bow_uni_tsvd\n","Assesing model KNeighborsClassifier on dataset bow_bi_tsvd\n","Assesing model KNeighborsClassifier on dataset tfidf_uni_lda\n","Assesing model KNeighborsClassifier on dataset tfidf_bi_lda\n","Assesing model KNeighborsClassifier on dataset tfidf_uni_tsvd\n","Assesing model KNeighborsClassifier on dataset tfidf_bi_tsvd\n","Assesing model KNeighborsClassifier on dataset w2v\n","Assesing model KNeighborsClassifier on dataset d2v\n","Assesing model KNeighborsClassifier on dataset glove\n","Assesing model Perceptron on dataset bow_uni_lda\n","Assesing model Perceptron on dataset bow_bi_lda\n","Assesing model Perceptron on dataset bow_uni_tsvd\n","Assesing model Perceptron on dataset bow_bi_tsvd\n","Assesing model Perceptron on dataset tfidf_uni_lda\n","Assesing model Perceptron on dataset tfidf_bi_lda\n","Assesing model Perceptron on dataset tfidf_uni_tsvd\n","Assesing model Perceptron on dataset tfidf_bi_tsvd\n","Assesing model Perceptron on dataset w2v\n","Assesing model Perceptron on dataset d2v\n","Assesing model Perceptron on dataset glove\n","Assesing model MLPClassifier on dataset bow_uni_lda\n","Assesing model MLPClassifier on dataset bow_bi_lda\n","Assesing model MLPClassifier on dataset bow_uni_tsvd\n","Assesing model MLPClassifier on dataset bow_bi_tsvd\n","Assesing model MLPClassifier on dataset tfidf_uni_lda\n","Assesing model MLPClassifier on dataset tfidf_bi_lda\n","Assesing model MLPClassifier on dataset tfidf_uni_tsvd\n","Assesing model MLPClassifier on dataset tfidf_bi_tsvd\n","Assesing model MLPClassifier on dataset w2v\n","Assesing model MLPClassifier on dataset d2v\n","Assesing model MLPClassifier on dataset glove\n"]}],"source":["accuracy_df, f1_df, params_df = run_eval_on_models(classical_models, vec_x_paths)"]},{"cell_type":"code","source":["accuracy_df = round(accuracy_df, 4)"],"metadata":{"id":"0CBLcx6XI6MT","executionInfo":{"status":"ok","timestamp":1688311177595,"user_tz":-120,"elapsed":20,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["np.array(accuracy_df.MLPClassifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9rrRZZAI-AM","executionInfo":{"status":"ok","timestamp":1688312453888,"user_tz":-120,"elapsed":258,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}},"outputId":"1cb29044-0b7c-447e-946e-c8627a9c53cf"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.6533, 0.6618, 0.8033, 0.8103, 0.559 , 0.5845,    nan, 0.8274,\n","       0.7837, 0.7707, 0.5083])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1687637357206,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"46xsMRk7cdPV","outputId":"25ca58d2-ef94-48aa-a69f-30dd0626224d"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-45c3765e-1692-4cb7-9714-275107952cd6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>LogisticRegression</th>\n","      <th>GaussianNB</th>\n","      <th>SVC</th>\n","      <th>DecisionTreeClassifier</th>\n","      <th>RandomForestClassifier</th>\n","      <th>LGBMClassifier</th>\n","      <th>KNeighborsClassifier</th>\n","      <th>Perceptron</th>\n","      <th>MLPClassifier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>bow_uni_lda</th>\n","      <td>0.629107</td>\n","      <td>0.606237</td>\n","      <td>0.648080</td>\n","      <td>0.606118</td>\n","      <td>0.680722</td>\n","      <td>0.677392</td>\n","      <td>0.656768</td>\n","      <td>0.391938</td>\n","      <td>0.653132</td>\n","    </tr>\n","    <tr>\n","      <th>bow_bi_lda</th>\n","      <td>0.632657</td>\n","      <td>0.615020</td>\n","      <td>0.644406</td>\n","      <td>0.628494</td>\n","      <td>0.693356</td>\n","      <td>0.675454</td>\n","      <td>0.657628</td>\n","      <td>0.341600</td>\n","      <td>0.661806</td>\n","    </tr>\n","    <tr>\n","      <th>bow_uni_tsvd</th>\n","      <td>0.799799</td>\n","      <td>0.483662</td>\n","      <td>NaN</td>\n","      <td>0.604563</td>\n","      <td>0.736363</td>\n","      <td>0.807273</td>\n","      <td>NaN</td>\n","      <td>0.758577</td>\n","      <td>0.803311</td>\n","    </tr>\n","    <tr>\n","      <th>bow_bi_tsvd</th>\n","      <td>0.815802</td>\n","      <td>0.477966</td>\n","      <td>NaN</td>\n","      <td>0.580973</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>tfidf_uni_lda</th>\n","      <td>0.490672</td>\n","      <td>0.475455</td>\n","      <td>0.441789</td>\n","      <td>0.540790</td>\n","      <td>0.545090</td>\n","      <td>0.555754</td>\n","      <td>0.552757</td>\n","      <td>0.329633</td>\n","      <td>0.537633</td>\n","    </tr>\n","    <tr>\n","      <th>tfidf_bi_lda</th>\n","      <td>0.355211</td>\n","      <td>0.355211</td>\n","      <td>0.354404</td>\n","      <td>0.568615</td>\n","      <td>0.572215</td>\n","      <td>0.580776</td>\n","      <td>0.565721</td>\n","      <td>0.336993</td>\n","      <td>0.583678</td>\n","    </tr>\n","    <tr>\n","      <th>tfidf_uni_tsvd</th>\n","      <td>0.804773</td>\n","      <td>0.637258</td>\n","      <td>0.793617</td>\n","      <td>0.620462</td>\n","      <td>0.705967</td>\n","      <td>0.758041</td>\n","      <td>0.704351</td>\n","      <td>0.777175</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>tfidf_bi_tsvd</th>\n","      <td>0.829750</td>\n","      <td>0.620551</td>\n","      <td>0.821846</td>\n","      <td>0.594429</td>\n","      <td>NaN</td>\n","      <td>0.769671</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>w2v</th>\n","      <td>0.754122</td>\n","      <td>0.639037</td>\n","      <td>0.729441</td>\n","      <td>0.649842</td>\n","      <td>0.746471</td>\n","      <td>0.776140</td>\n","      <td>0.719969</td>\n","      <td>0.726031</td>\n","      <td>0.783153</td>\n","    </tr>\n","    <tr>\n","      <th>d2v</th>\n","      <td>0.733562</td>\n","      <td>0.615813</td>\n","      <td>0.778111</td>\n","      <td>0.575373</td>\n","      <td>0.724648</td>\n","      <td>0.760647</td>\n","      <td>0.676388</td>\n","      <td>0.659547</td>\n","      <td>0.770446</td>\n","    </tr>\n","    <tr>\n","      <th>glove</th>\n","      <td>0.336993</td>\n","      <td>0.504734</td>\n","      <td>0.496227</td>\n","      <td>0.498746</td>\n","      <td>0.504244</td>\n","      <td>0.515239</td>\n","      <td>0.510473</td>\n","      <td>0.329633</td>\n","      <td>0.336993</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45c3765e-1692-4cb7-9714-275107952cd6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-45c3765e-1692-4cb7-9714-275107952cd6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-45c3765e-1692-4cb7-9714-275107952cd6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                LogisticRegression  GaussianNB       SVC  \\\n","bow_uni_lda               0.629107    0.606237  0.648080   \n","bow_bi_lda                0.632657    0.615020  0.644406   \n","bow_uni_tsvd              0.799799    0.483662       NaN   \n","bow_bi_tsvd               0.815802    0.477966       NaN   \n","tfidf_uni_lda             0.490672    0.475455  0.441789   \n","tfidf_bi_lda              0.355211    0.355211  0.354404   \n","tfidf_uni_tsvd            0.804773    0.637258  0.793617   \n","tfidf_bi_tsvd             0.829750    0.620551  0.821846   \n","w2v                       0.754122    0.639037  0.729441   \n","d2v                       0.733562    0.615813  0.778111   \n","glove                     0.336993    0.504734  0.496227   \n","\n","                DecisionTreeClassifier  RandomForestClassifier  \\\n","bow_uni_lda                   0.606118                0.680722   \n","bow_bi_lda                    0.628494                0.693356   \n","bow_uni_tsvd                  0.604563                0.736363   \n","bow_bi_tsvd                   0.580973                     NaN   \n","tfidf_uni_lda                 0.540790                0.545090   \n","tfidf_bi_lda                  0.568615                0.572215   \n","tfidf_uni_tsvd                0.620462                0.705967   \n","tfidf_bi_tsvd                 0.594429                     NaN   \n","w2v                           0.649842                0.746471   \n","d2v                           0.575373                0.724648   \n","glove                         0.498746                0.504244   \n","\n","                LGBMClassifier  KNeighborsClassifier  Perceptron  \\\n","bow_uni_lda           0.677392              0.656768    0.391938   \n","bow_bi_lda            0.675454              0.657628    0.341600   \n","bow_uni_tsvd          0.807273                   NaN    0.758577   \n","bow_bi_tsvd                NaN                   NaN         NaN   \n","tfidf_uni_lda         0.555754              0.552757    0.329633   \n","tfidf_bi_lda          0.580776              0.565721    0.336993   \n","tfidf_uni_tsvd        0.758041              0.704351    0.777175   \n","tfidf_bi_tsvd         0.769671                   NaN         NaN   \n","w2v                   0.776140              0.719969    0.726031   \n","d2v                   0.760647              0.676388    0.659547   \n","glove                 0.515239              0.510473    0.329633   \n","\n","                MLPClassifier  \n","bow_uni_lda          0.653132  \n","bow_bi_lda           0.661806  \n","bow_uni_tsvd         0.803311  \n","bow_bi_tsvd               NaN  \n","tfidf_uni_lda        0.537633  \n","tfidf_bi_lda         0.583678  \n","tfidf_uni_tsvd            NaN  \n","tfidf_bi_tsvd             NaN  \n","w2v                  0.783153  \n","d2v                  0.770446  \n","glove                0.336993  "]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["f1_df #round(f1_df, 2).to_latex()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4m-eQ0tZrfqv"},"outputs":[],"source":["def run_eval_on_models2(models, datasets_paths, req_params):\n","    pattern = r'(?<=vec/)(.*?)(?=_test_x.parquet.gzip|_train_x.parquet.gzip)'\n","\n","    for i, model in enumerate(models):\n","        for j, datasets in enumerate(datasets_paths):\n","            datasets.sort()\n","            temp_model_name = str(model).split('.')[-1].split(\"'\")[0]\n","            temp_data_name = re.search(pattern, datasets[0]).group(0)\n","            print(f\"Assesing model {temp_model_name} on dataset {temp_data_name}\")\n","\n","            # Load best model - if it exists:\n","            try:\n","                filename = f\"{path}ECHR_model/model_{temp_model_name}__dataset_{temp_data_name}.pkl\"\n","                loaded_model = pickle.load(open(filename, 'rb'))\n","                params = loaded_model.get_params()\n","                params = {k: params[k] for k in req_params}\n","            except:\n","                params = np.nan\n","\n","            print(params)\n","            print()"]},{"cell_type":"code","source":["params = [\n","    # { # LogisticRegression\n","    # 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n","    # 'penalty': [None, 'l1', 'l2'],\n","    # 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n","    # },\n","    # { # GaussianNB\n","    # 'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n","    # },\n","    # { # SVC\n","    # 'C': [0.001, 0.01, 0.1, 1, 10],\n","    # 'gamma': [1, 0.1, 0.01, 0.001],\n","    # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n","    # },\n","    # { # DecisionTreeClassifier\n","    # 'criterion': ['gini', 'entropy', 'log_loss'],\n","    # 'max_depth': np.arange(10, 50, 2),\n","    # 'splitter': ['best', 'random'],\n","    # 'ccp_alpha': np.arange(0, 0.2, 0.01),\n","    # },\n","    # { # RandomForestClassifier\n","    # 'n_estimators': np.arange(10, 200, 10),\n","    # 'criterion': ['gini', 'entropy', 'log_loss'],\n","    # 'max_depth': np.arange(10, 50, 2),\n","    # 'ccp_alpha': np.arange(0, 0.2, 0.01),\n","    # },\n","    # { # LGBMClassifier\n","    # 'boosting_type': ['gbdt', 'dart', 'goss'],\n","    # 'num_leaves': [10, 50, 100, 200],\n","    # 'max_depth': [5, 10, 15, 20, 50],\n","    # 'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","    # 'n_estimators': [100, 200, 500, 1000],\n","    # 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n","    # 'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    # },\n","    # { # KNeighborsClassifier\n","    # 'n_neighbors': np.arange(10, 50, 2),\n","    # 'weights': ['uniform', 'distance'],\n","    # 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n","    # 'p': [1, 2, 3]\n","    # },\n","    # { # Perceptron\n","    # 'penalty': [None, 'l1', 'l2', 'elasticnet'],\n","    # 'alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n","    # 'max_iter': np.arange(1000, 5000, 1000),\n","    # },\n","    { # MLPClassifier\n","    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n","    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n","    'solver': ['lbfgs', 'sgd', 'adam'],\n","    'alpha': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n","    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n","    }\n","    ][0]"],"metadata":{"id":"HSBZdviWAMKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classical_models = [\n","    # LogisticRegression,\n","    # GaussianNB,\n","    # SVC,\n","    # DecisionTreeClassifier,\n","    # RandomForestClassifier,\n","    # LGBMClassifier,\n","    # KNeighborsClassifier,\n","    # Perceptron,\n","    MLPClassifier,\n","    ]\n","run_eval_on_models2(classical_models, vec_x_paths, list(params.keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KB5E9j-P_ACm","executionInfo":{"status":"ok","timestamp":1688044352278,"user_tz":-120,"elapsed":10,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}},"outputId":"e2be3006-39b7-40d2-a3cc-a06190c79216"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Assesing model MLPClassifier on dataset bow_uni_lda\n","{'hidden_layer_sizes': (50, 50, 50), 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.001, 'learning_rate': 'invscaling'}\n","\n","Assesing model MLPClassifier on dataset bow_bi_lda\n","{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'alpha': 1e-05, 'learning_rate': 'adaptive'}\n","\n","Assesing model MLPClassifier on dataset bow_uni_tsvd\n","{'hidden_layer_sizes': (50, 100, 50), 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.001, 'learning_rate': 'adaptive'}\n","\n","Assesing model MLPClassifier on dataset bow_bi_tsvd\n","{'hidden_layer_sizes': (50, 100, 50), 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.001, 'learning_rate': 'adaptive'}\n","\n","Assesing model MLPClassifier on dataset tfidf_uni_lda\n","{'hidden_layer_sizes': (50, 50, 50), 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 1e-05, 'learning_rate': 'invscaling'}\n","\n","Assesing model MLPClassifier on dataset tfidf_bi_lda\n","{'hidden_layer_sizes': (50, 100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 1e-06, 'learning_rate': 'adaptive'}\n","\n","Assesing model MLPClassifier on dataset tfidf_uni_tsvd\n","nan\n","\n","Assesing model MLPClassifier on dataset tfidf_bi_tsvd\n","{'hidden_layer_sizes': (50, 50, 50), 'activation': 'identity', 'solver': 'sgd', 'alpha': 0.01, 'learning_rate': 'constant'}\n","\n","Assesing model MLPClassifier on dataset w2v\n","{'hidden_layer_sizes': (50, 100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 1e-06, 'learning_rate': 'adaptive'}\n","\n","Assesing model MLPClassifier on dataset d2v\n","{'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'alpha': 1, 'learning_rate': 'invscaling'}\n","\n","Assesing model MLPClassifier on dataset glove\n","{'hidden_layer_sizes': (50, 50, 50), 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.01, 'learning_rate': 'invscaling'}\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"hmP-v92o-ozn"},"source":["## Size Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UWnbbfcNMZWx"},"outputs":[],"source":["vec_x_paths = [\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_test_x.parquet.gzip'],\n","    #['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_lda_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_lda_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_uni_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/tfidf_bi_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_uni_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_tsvd_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/bow_bi_tsvd_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/w2v_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/w2v_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/d2v_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/d2v_test_x.parquet.gzip'],\n","    ['drive/MyDrive/Thesis/ECHR_Dataset_vec/glove_train_x.parquet.gzip', 'drive/MyDrive/Thesis/ECHR_Dataset_vec/glove_test_x.parquet.gzip'],\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3cjSPwQMaol"},"outputs":[],"source":["def run_size_eval(datasets_paths):\n","    pattern = r'(?<=vec/)(.*?)(?=_test_x.parquet.gzip|_train_x.parquet.gzip)'\n","\n","    for j, datasets in enumerate(datasets_paths):\n","        datasets.sort()\n","        temp_data_name = re.search(pattern, datasets[0]).group(0)\n","        print(f\"Assesing dataset {temp_data_name}\")\n","\n","        # Read correct data:\n","        x_train = pd.read_parquet(datasets[1])\n","        x_test = pd.read_parquet(datasets[0])\n","\n","        x_df = pd.concat([x_train, x_test], axis=0)\n","        print(f\"{x_df.shape[1]} columns\")\n","        print(f\"{x_df.memory_usage(deep=True).sum()/1000000} MB\")\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54065,"status":"ok","timestamp":1687632882002,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"PMP3NHUsM4o7","outputId":"dd9fd464-3186-460a-eba9-9f53d19e16b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Assesing dataset tfidf_uni\n","35475 columns\n","2827.862912 MB\n","\n","Assesing dataset bow_uni\n","35475 columns\n","707.025512 MB\n","\n","Assesing dataset bow_uni_lda\n","35 columns\n","2.869632 MB\n","\n","Assesing dataset bow_bi_lda\n","35 columns\n","2.869632 MB\n","\n","Assesing dataset tfidf_uni_lda\n","5 columns\n","0.278992 MB\n","\n","Assesing dataset tfidf_bi_lda\n","5 columns\n","0.278992 MB\n","\n","Assesing dataset tfidf_uni_tsvd\n","1940 columns\n","77.400352 MB\n","\n","Assesing dataset tfidf_bi_tsvd\n","3000 columns\n","119.647712 MB\n","\n","Assesing dataset bow_uni_tsvd\n","700 columns\n","27.978912 MB\n","\n","Assesing dataset bow_bi_tsvd\n","505 columns\n","20.206992 MB\n","\n","Assesing dataset w2v\n","100 columns\n","4.065312 MB\n","\n","Assesing dataset d2v\n","100 columns\n","4.065312 MB\n","\n","Assesing dataset glove\n","300 columns\n","12.036512 MB\n","\n"]}],"source":["run_size_eval(vec_x_paths)"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["ABmVdjDYQDZ5"],"machine_shape":"hm","authorship_tag":"ABX9TyNshP4Rsmsd38qHlcZuhW3D"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}