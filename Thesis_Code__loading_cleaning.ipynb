{"cells":[{"cell_type":"markdown","metadata":{"id":"a4b5da75-708d-442f-b2e5-a35db4dcd770"},"source":["# Packages:"],"id":"a4b5da75-708d-442f-b2e5-a35db4dcd770"},{"cell_type":"code","execution_count":1,"metadata":{"id":"aFNsRyLQkgzL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688335341335,"user_tz":-120,"elapsed":30026,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}},"outputId":"e5cee2bd-218a-4ecb-ea30-74cf857bafe6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: nvidia-smi: command not found\n","Mounted at /content/drive\n"]}],"source":["try:\n","  from google.colab import drive\n","  !nvidia-smi\n","  drive.mount('/content/drive')\n","  path = 'drive/MyDrive/Thesis/'\n","except:\n","  path = './'"],"id":"aFNsRyLQkgzL"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1837,"status":"ok","timestamp":1688335343169,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"},"user_tz":-120},"id":"c3d0caca-2f9e-4e45-9f29-de0ddcb91f0b"},"outputs":[],"source":["# Packages for loading data:\n","from os import walk\n","import os\n","import pprint\n","import itertools\n","import json\n","import re\n","import pickle\n","import sys\n","import warnings\n","\n","# Packages for effective data storage / math utils:\n","import pandas as pd\n","import numpy as np\n","\n","# Packages for plotting:\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Packages for data cleaning:\n","import string\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","# Packages for test train data prep:\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import train_test_split\n","\n","# Packages for text representation:\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Misc.:\n","import time\n","import multiprocessing\n","\n","seed = 101\n","cores = multiprocessing.cpu_count()"],"id":"c3d0caca-2f9e-4e45-9f29-de0ddcb91f0b"},{"cell_type":"markdown","metadata":{"id":"6274eaeb-2669-4b16-be45-ffa714fb38b2"},"source":["# Text Loading:"],"id":"6274eaeb-2669-4b16-be45-ffa714fb38b2"},{"cell_type":"code","execution_count":3,"metadata":{"id":"7c4f7412-c629-4e3b-b873-7acea3b99305","executionInfo":{"status":"ok","timestamp":1688335343170,"user_tz":-120,"elapsed":12,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}}},"outputs":[],"source":["def file_path_getter(\n","    filepath : str,\n","    foldername : str\n","):\n","    #########\n","    # Input: filepath, path from notebook to raw data jsons\n","    # Output: lost of all paths to jasons to be used later\n","    #########\n","\n","    filenames = next(walk(filepath + \"/\" + foldername), (None, None, []))[2]\n","    filenames = [str(filepath + \"/\" + foldername + \"/\" + file) for file in filenames]\n","\n","    return(filenames)\n","\n","filenames_train = file_path_getter(\"./ECHR_Dataset\", \"EN_train\")\n","filenames_test = file_path_getter(\"./ECHR_Dataset\", \"EN_test\")\n","filenames_dev = file_path_getter(\"./ECHR_Dataset\", \"EN_dev\")\n","\n","filenames_all = set(itertools.chain(filenames_train, filenames_test, filenames_dev))"],"id":"7c4f7412-c629-4e3b-b873-7acea3b99305"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8980361-4334-4695-a863-9716c09c43c7"},"outputs":[],"source":["cases_all = []\n","for filename in filenames_all:\n","    if \"DS_Store\" in filename:\n","        continue\n","\n","    with open(filename) as f:\n","        data_temp = json.load(f)\n","    data_temp['TEXT'] = \" \".join(data_temp['TEXT'])\n","\n","    rem_list = [\"VIOLATED_ARTICLES\", \"VIOLATED_PARAGRAPHS\", \"VIOLATED_BULLETPOINTS\",\n","               \"NON_VIOLATED_ARTICLES\", \"NON_VIOLATED_PARAGRAPHS\", \"NON_VIOLATED_BULLETPOINTS\",]\n","    for key in rem_list:\n","        del data_temp[key]\n","\n","    cases_all.append(data_temp)\n","\n","cases_df_raw = pd.DataFrame(cases_all)\n","cases_df_raw['Num_words_text'] = cases_df_raw['TEXT'].apply(lambda x:len(str(x).split()))"],"id":"a8980361-4334-4695-a863-9716c09c43c7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e799647-473c-465a-b5fe-0c7e7328a2ef","executionInfo":{"status":"aborted","timestamp":1688335343172,"user_tz":-120,"elapsed":11,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}}},"outputs":[],"source":["new_CONCLUSION = []\n","for i in cases_df_raw.CONCLUSION:\n","    i = i.lower()\n","    if bool(re.search(\"(^violation)|(;violation)\", i)):\n","        new_CONCLUSION.append(1)\n","\n","    elif bool(re.search(\"^inadmissible\", i)):\n","        new_CONCLUSION.append(0)\n","\n","    elif (\n","        bool(re.search(\"^no violation\", i)) or\n","        bool(re.search(\"^remainder inadmissible\", i)) or\n","        bool(re.search(\"^inapplicable\", i)) or\n","        bool(re.search(\"^lack of jurisdiction\", i)) or\n","        bool(re.search(\"(?:^preliminary objection)(.*)(?:allow)\", i)) or\n","        bool(re.search(\"(?:^preliminary objection)(.*)(?:dismiss)\", i)) or\n","        bool(re.search(\"(?:^preliminary objection)(.*)(?:merit)\", i)) or\n","        bool(re.search(\"(?:^preliminary objection)(.*)(?:reject)\", i)) or\n","        bool(re.search(\"revision rejected\", i)) or\n","        bool(re.search(\"pecuniary\", i))\n","    ):\n","        new_CONCLUSION.append(0) #no violation\n","\n","    else:\n","        new_CONCLUSION.append(np.nan)\n","cases_df_raw['new_CONCLUSION'] = np.array(new_CONCLUSION)\n","cases_df_raw = cases_df_raw.dropna(axis = 0, how = \"any\")\n","\n","RUS = RandomUnderSampler(\n","    sampling_strategy='all',\n","    random_state=seed,\n","    replacement=False,\n",")\n","\n","X_rus, Y_rus = RUS.fit_resample(cases_df_raw.loc[:, cases_df_raw.columns != 'new_CONCLUSION'], cases_df_raw.new_CONCLUSION)\n","cases_df_raw = pd.concat([X_rus.reset_index(drop=True), Y_rus], axis=1)\n","\n","judges_dummy_df = pd.get_dummies(cases_df_raw.JUDGES.str.split(';').explode()).groupby(level=0).sum()\n","cases_df_raw = pd.concat([cases_df_raw.reset_index(drop=True), judges_dummy_df], axis=1)\n","cases_df_raw = cases_df_raw.loc[:, cases_df_raw.columns != 'JUDGES']\n","\n","for col in cases_df_raw.columns:\n","    if len(cases_df_raw[col].unique()) == 1:\n","        cases_df_raw.drop(col, inplace=True, axis=1)"],"id":"9e799647-473c-465a-b5fe-0c7e7328a2ef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4a5547d-cfad-4fdf-b4a8-e30b18fccb80"},"outputs":[],"source":["cases_df_raw.to_parquet(\"./ECHR_Dataset_clean/data_raw.parquet.gzip\", compression='gzip', index = False)"],"id":"a4a5547d-cfad-4fdf-b4a8-e30b18fccb80"},{"cell_type":"markdown","metadata":{"id":"a3300818-d705-4d5d-94fe-3d99b53e4a7e"},"source":["# Text Cleaning:"],"id":"a3300818-d705-4d5d-94fe-3d99b53e4a7e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d231dc3-5ccb-4274-8974-c9980a42dd14"},"outputs":[],"source":["cases_df_raw = pd.read_parquet(\"./ECHR_Dataset_clean/data_raw.parquet.gzip\")\n","cases_df_clean = cases_df_raw.copy()"],"id":"6d231dc3-5ccb-4274-8974-c9980a42dd14"},{"cell_type":"code","execution_count":6,"metadata":{"id":"daed6563-ed55-4ff9-8531-b357134738ac","executionInfo":{"status":"ok","timestamp":1688335371855,"user_tz":-120,"elapsed":2,"user":{"displayName":"Peter Novak","userId":"17982974517295573360"}}},"outputs":[],"source":["def remove_Stopwords(text):\n","    stop_words = set(stopwords.words('english'))\n","    words = word_tokenize( text.lower() )\n","    sentence = [w for w in words if not w in stop_words]\n","    return \" \".join(sentence)\n","\n","def lemmatize_text(text):\n","    wordlist = []\n","    lemmatizer = WordNetLemmatizer()\n","    sentences = sent_tokenize(text)\n","    for sentence in sentences:\n","        words = word_tokenize(sentence)\n","        for word in words:\n","            wordlist.append(lemmatizer.lemmatize(word))\n","    return \" \".join(wordlist)\n","\n","def clean_text(text):\n","    delete_dict = {sp_character: '' for sp_character in string.punctuation}\n","    delete_dict[\" \"] = \" \"\n","    table = str.maketrans(delete_dict)\n","    text1 = text.translate(table)\n","    textArr = text1.split()\n","    text2 = \" \".join([w for w in textArr])\n","\n","    return text2.lower()"],"id":"daed6563-ed55-4ff9-8531-b357134738ac"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf59efc7-8a95-4732-89ad-ca49c3d40244"},"outputs":[],"source":["cases_df_clean['TEXT'] = cases_df_clean['TEXT'].apply(clean_text)\n","cases_df_clean['TEXT'] = cases_df_clean['TEXT'].apply(remove_Stopwords)\n","cases_df_clean['TEXT'] = cases_df_clean['TEXT'].apply(lemmatize_text)\n","cases_df_clean['Num_words_text'] = cases_df_clean['TEXT'].apply(lambda x:len(str(x).split()))"],"id":"bf59efc7-8a95-4732-89ad-ca49c3d40244"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a700fe5-9f1e-4284-b5e7-033daa6ff494"},"outputs":[],"source":["df_train, df_test = train_test_split(cases_df_clean, test_size=0.2)\n","df_train_x = df_train.TEXT\n","df_train_y = df_train.new_CONCLUSION\n","df_test_x = df_test.TEXT\n","df_test_y = df_test.new_CONCLUSION"],"id":"6a700fe5-9f1e-4284-b5e7-033daa6ff494"},{"cell_type":"code","execution_count":null,"metadata":{"id":"482159ca-cd95-4482-bcd8-ee45bac25955"},"outputs":[],"source":["df_train_x.to_pickle(\"./ECHR_Dataset_clean/df_train_x.pkl\")\n","df_train_y.to_pickle(\"./ECHR_Dataset_clean/df_train_y.pkl\")\n","df_test_x.to_pickle(\"./ECHR_Dataset_clean/df_test_x.pkl\")\n","df_test_y.to_pickle(\"./ECHR_Dataset_clean/df_test_y.pkl\")"],"id":"482159ca-cd95-4482-bcd8-ee45bac25955"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0471b0bc-5944-4b3b-b3db-3e18c39610c2"},"outputs":[],"source":["fig, axs = plt.subplots(1,2, sharex = True, sharey = True, figsize = (12, 7))\n","\n","# color bars by outcome\n","cases_df_raw['Num_words_text'].plot(kind = \"hist\", bins=100, title='Raw - Word Count Distribution', ax = axs[0])\n","cases_df_clean['Num_words_text'].plot(kind = \"hist\", bins=100, title='Clean - Word Count Distribution', ax = axs[1])\n","plt.show()"],"id":"0471b0bc-5944-4b3b-b3db-3e18c39610c2"},{"cell_type":"markdown","metadata":{"id":"461ac332-f475-4fa4-bf27-039f9265e17b"},"source":["## Distribution of Unigrams, Bigrams, Trigrams in clean vs raw:"],"id":"461ac332-f475-4fa4-bf27-039f9265e17b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"122b3867-1da4-4c20-a5de-0759eea841bc"},"outputs":[],"source":["def get_top_n_x_gram(corpus, x = 1, n=None):\n","    vec = CountVectorizer(ngram_range=(x, x)).fit(corpus)\n","    bag_of_words = vec.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0)\n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","    return words_freq[:n]"],"id":"122b3867-1da4-4c20-a5de-0759eea841bc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"20f521f1-04d8-4e2e-93a6-fba24937def5"},"outputs":[],"source":["fig, axs = plt.subplots(1,2, sharey = True, figsize = (12, 7))\n","\n","common_words = get_top_n_x_gram(cases_df_raw['TEXT'], 1, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Unigrams - Raw - Top 20 words', ax = axs[0])\n","\n","common_words = get_top_n_x_gram(cases_df_clean['TEXT'], 1, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Unigrams - Clean - Top 20 words', ax = axs[1])\n","plt.show()"],"id":"20f521f1-04d8-4e2e-93a6-fba24937def5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"076b9d02-f6ee-4188-aaa6-d52c04a2782c"},"outputs":[],"source":["fig, axs = plt.subplots(1,2, sharey = True, figsize = (12, 7))\n","\n","common_words = get_top_n_x_gram(cases_df_raw['TEXT'], 2, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Bigrams - Raw - Top 20 words', ax = axs[0])\n","\n","common_words = get_top_n_x_gram(cases_df_clean['TEXT'], 2, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Bigrams - Clean - Top 20 words', ax = axs[1])\n","plt.show()"],"id":"076b9d02-f6ee-4188-aaa6-d52c04a2782c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"18020d7a-acfb-4c9f-8736-d68590901c90"},"outputs":[],"source":["fig, axs = plt.subplots(1,2, sharey = True, figsize = (12, 7))\n","\n","common_words = get_top_n_x_gram(cases_df_raw['TEXT'], 3, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Trigrams - Raw - Top 20 words', ax = axs[0])\n","\n","common_words = get_top_n_x_gram(cases_df_clean['TEXT'], 3, 20)\n","df1 = pd.DataFrame(common_words, columns = ['TEXT', 'count'])\n","df1.groupby('TEXT').sum()['count'].sort_values(ascending=False).plot(kind='bar', title='Trigrams - Clean - Top 20 words', ax = axs[1])\n","plt.show()"],"id":"18020d7a-acfb-4c9f-8736-d68590901c90"}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}